{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b1db647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af382105",
   "metadata": {},
   "source": [
    "# BASNET \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079857d7",
   "metadata": {},
   "source": [
    "**Basic Architecture of BASNet consist of two Module, Prediction Module and Refined Module**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f03033d",
   "metadata": {},
   "source": [
    "- lets start with prediction Module\n",
    "- Construct helper function \n",
    "- The Prediction module is U-Net like structure (encoder-decoder)\n",
    "- For Encoder we are going to use residual blocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "058504cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class downsample(tf.keras.Model):\n",
    "    '''\n",
    "    Used to match the layer shapes for addition \n",
    "    in basicblock\n",
    "    '''\n",
    "    def __init__(self,filters,stride=1):\n",
    "        super(downsample,self).__init__()\n",
    "        self.conv = layers.Conv2D(filters,1,strides=stride,padding='same',use_bias=False)\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.relu = layers.Activation('relu')\n",
    "        \n",
    "    def call(self,x):\n",
    "        res = x \n",
    "        x = self.conv(res)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab448e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class basicBlock(tf.keras.Model):\n",
    "    '''\n",
    "    Basic Residual Block\n",
    "    '''\n",
    "    def __init__(self,filters,stride=1,downsamples=False):\n",
    "        super(basicBlock,self).__init__()\n",
    "        self.conv1 = layers.Conv2D(filters,3,strides = stride,padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.Activation('relu')\n",
    "        self.conv2 = layers.Conv2D(filters,3,strides=1,padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.relu2 = layers.Activation('relu')\n",
    "        self.downsamples = downsamples\n",
    "        if self.downsamples:\n",
    "            self.dsmp = downsample(filters,stride)\n",
    "        \n",
    "    def call(self,x_in):\n",
    "        res = x_in\n",
    "        x = self.conv1(res)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.downsamples:\n",
    "            res = self.dsmp(res)\n",
    "        x = layers.Add()([x,res])\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "779c772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class reslayer1(tf.keras.Model):\n",
    "    '''\n",
    "    Resnet34 Layer 1 \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(reslayer1,self).__init__()\n",
    "        self.convblock1 = basicBlock(64)\n",
    "        self.convblock2 = basicBlock(64)\n",
    "        self.convblock3 = basicBlock(64)\n",
    "        \n",
    "    def call(self,x_in):\n",
    "        x = self.convblock1(x_in)\n",
    "        x =  self.convblock2(x)\n",
    "        x = self.convblock3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f104d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class reslayer2(tf.keras.Model):\n",
    "    '''\n",
    "    Resnet34 Layer 2\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(reslayer2,self).__init__()\n",
    "        self.convblock1 = basicBlock(128,2,downsamples=True)\n",
    "        self.convblock2 = basicBlock(128)\n",
    "        self.convblock3 = basicBlock(128)\n",
    "        self.convblock4 = basicBlock(128)\n",
    "        \n",
    "    def call(self,x_in):\n",
    "        x = self.convblock1(x_in)\n",
    "        x =  self.convblock2(x)\n",
    "        x = self.convblock3(x)\n",
    "        x = self.convblock4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe06951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class reslayer3(tf.keras.Model):\n",
    "    '''\n",
    "    Resnet34 Layer 3 \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(reslayer3,self).__init__()\n",
    "        self.convblock1 = basicBlock(256,2,downsamples=True)\n",
    "        self.convblock2 = basicBlock(256)\n",
    "        self.convblock3 = basicBlock(256)\n",
    "        self.convblock4 = basicBlock(256)\n",
    "        self.convblock5 = basicBlock(256)\n",
    "        self.convblock6 = basicBlock(256)\n",
    "        \n",
    "    def call(self,x_in):\n",
    "        x = self.convblock1(x_in)\n",
    "        x =  self.convblock2(x)\n",
    "        x = self.convblock3(x)\n",
    "        x = self.convblock4(x)\n",
    "        x = self.convblock5(x)\n",
    "        x = self.convblock6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f26eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class reslayer4(tf.keras.Model):\n",
    "    '''\n",
    "    Resnet34 Layer 4 \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(reslayer4,self).__init__()\n",
    "        self.convblock1 = basicBlock(512,2,downsamples=True)\n",
    "        self.convblock2 = basicBlock(512)\n",
    "        self.convblock3 = basicBlock(512)\n",
    "        \n",
    "    def call(self,x_in):\n",
    "        x = self.convblock1(x_in)\n",
    "        x =  self.convblock2(x)\n",
    "        x = self.convblock3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b153ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefUnet(tf.keras.Model):\n",
    "    '''\n",
    "    Residual Refinement Module  \n",
    "    '''\n",
    "    def __init__(self,filters):\n",
    "        super(RefUnet,self).__init__()\n",
    "        self.conv0 = layers.Conv2D(filters,3,padding='same')\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(64,3,padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.Activation('relu')\n",
    "        \n",
    "        self.pool1=layers.MaxPool2D(strides=(2, 2))\n",
    "        \n",
    "        self.conv2 = layers.Conv2D(64,3,padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.relu2 = layers.Activation('relu')\n",
    "        \n",
    "        self.pool2=layers.MaxPool2D(strides=(2, 2))\n",
    "        \n",
    "        self.conv3 = layers.Conv2D(64,3,padding='same')\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.relu3 = layers.Activation('relu')\n",
    "        \n",
    "        self.pool3=layers.MaxPool2D(strides=(2, 2))\n",
    "        \n",
    "        self.conv4 = layers.Conv2D(64,3,padding='same')\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        self.relu4 = layers.Activation('relu')\n",
    "        \n",
    "        self.pool4=layers.MaxPool2D(strides=(2, 2))\n",
    "        ######\n",
    "        self.conv5 = layers.Conv2D(64,3,padding='same')\n",
    "        self.bn5 = layers.BatchNormalization()\n",
    "        self.relu5 = layers.Activation('relu')\n",
    "        \n",
    "        ####\n",
    "        \n",
    "        self.convd_4 = layers.Conv2D(64,3,padding='same')\n",
    "        self.bnd_4 = layers.BatchNormalization()\n",
    "        self.relud_4 = layers.Activation('relu')\n",
    "        \n",
    "        self.convd_3 = layers.Conv2D(64,3,padding='same')\n",
    "        self.bnd_3 = layers.BatchNormalization()\n",
    "        self.relud_3 = layers.Activation('relu')\n",
    "        \n",
    "        self.convd_2 = layers.Conv2D(64,3,padding='same')\n",
    "        self.bnd_2 = layers.BatchNormalization()\n",
    "        self.relud_2 = layers.Activation('relu')\n",
    "        \n",
    "        self.convd_1 = layers.Conv2D(64,3,padding='same')\n",
    "        self.bnd_1 = layers.BatchNormalization()\n",
    "        self.relud_1  = layers.Activation('relu')\n",
    "        \n",
    "        self.convd_0 = layers.Conv2D(1,3,padding='same')\n",
    "        self.upscore2 = layers.UpSampling2D(2,interpolation='bilinear')\n",
    "        \n",
    "    def call(self,x):\n",
    "        hx = x\n",
    "        hx = self.conv0(hx)\n",
    "        \n",
    "        hx1 = self.relu1(self.bn1(self.conv1(hx)))\n",
    "        hx = self.pool1(hx1)\n",
    "        \n",
    "        hx2 = self.relu2(self.bn2(self.conv2(hx)))\n",
    "        hx = self.pool2(hx2)\n",
    "        \n",
    "        hx3 = self.relu3(self.bn3(self.conv3(hx)))\n",
    "        hx = self.pool3(hx3)\n",
    "        \n",
    "        hx4 = self.relu4(self.bn4(self.conv4(hx)))\n",
    "        hx = self.pool4(hx4)\n",
    "        \n",
    "        hx5 = self.relu5(self.bn5(self.conv5(hx)))\n",
    "        \n",
    "        \n",
    "        hx = self.upscore2(hx5)\n",
    "        \n",
    "        \n",
    "        d4 = self.relud_4(self.bnd_4(self.convd_4(layers.concatenate([hx,hx4],-1))))\n",
    "        hx = self.upscore2(d4)\n",
    "        \n",
    "        d3 = self.relud_3(self.bnd_3(self.convd_3(layers.concatenate([hx,hx3],-1))))\n",
    "        hx = self.upscore2(d3)\n",
    "        \n",
    "        d2 = self.relud_2(self.bnd_2(self.convd_2(layers.concatenate([hx,hx2],-1))))\n",
    "        hx = self.upscore2(d2)\n",
    "        \n",
    "        d1 = self.relud_1(self.bnd_1(self.convd_1(layers.concatenate([hx,hx1],-1))))\n",
    "        \n",
    "        residual = self.convd_0(d1)\n",
    "        \n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "320021b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasNet(tf.keras.Model):\n",
    "    '''\n",
    "    BasNet Model\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(BasNet,self).__init__()\n",
    "        #------------Encoeder------------------\n",
    "        \n",
    "        self.inconv = layers.Conv2D(64,3,strides = 1,padding='same')\n",
    "        self.inbn = layers.BatchNormalization()\n",
    "        self.inrelu = layers.Activation('relu')\n",
    "        \n",
    "        #stage 1 \n",
    "        self.encoder1 = reslayer1()#64\n",
    "        #stage 2 \n",
    "        self.encoder2 = reslayer2()#128\n",
    "        #stage 3\n",
    "        self.encoder3 = reslayer3()#256\n",
    "        #stage 4\n",
    "        self.encoder4 = reslayer4()#512\n",
    "        \n",
    "        \n",
    "        \n",
    "        #stage 5\n",
    "        self.pool4 = layers.MaxPool2D(strides=(2,2))\n",
    "        self.resb5_1 = basicBlock(512,downsamples=True)\n",
    "        self.resb5_2 = basicBlock(512)\n",
    "        self.resb5_3 = basicBlock(512)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #stage 6\n",
    "        self.pool5 = layers.MaxPool2D(strides=(2,2))\n",
    "        self.resb6_1 = basicBlock(512)\n",
    "        self.resb6_2 = basicBlock(512)\n",
    "        self.resb6_3 = basicBlock(512)\n",
    "        \n",
    "        \n",
    "        #----------------Bridge------------------------\n",
    "        #stage bridge\n",
    "        self.convbg_1 = layers.Conv2D(512,3,dilation_rate=2,padding='same')\n",
    "        self.bnbg_1 = layers.BatchNormalization()\n",
    "        self.relubg_1 = layers.Activation('relu')\n",
    "        self.convbg_2 = layers.Conv2D(512,3,dilation_rate=2,padding='same')\n",
    "        self.bnbg_2 = layers.BatchNormalization()\n",
    "        self.relubg_2 = layers.Activation('relu')\n",
    "        self.convbg_3 = layers.Conv2D(512,3,dilation_rate=2,padding='same')\n",
    "        self.bnbg_3 = layers.BatchNormalization()\n",
    "        self.relubg_3 = layers.Activation('relu')\n",
    "        \n",
    "        #-------------------Decoder----------------------\n",
    "        \n",
    "        #stage 6d\n",
    "        self.convd6_1 = layers.Conv2D(512,3,padding='same')\n",
    "        self.bnd6_1 = layers.BatchNormalization()\n",
    "        self.relud6_1 = layers.Activation('relu')\n",
    "        \n",
    "        self.convd6_2 = layers.Conv2D(512,3,dilation_rate=2,padding='same')\n",
    "        self.bnd6_2 = layers.BatchNormalization()\n",
    "        self.relud6_2 = layers.Activation('relu')\n",
    "        \n",
    "        self.convd6_3 = layers.Conv2D(512,3,dilation_rate=2,padding='same')\n",
    "        self.bnd6_3 = layers.BatchNormalization()\n",
    "        self.relud6_3 = layers.Activation('relu')\n",
    "        \n",
    "        #stage 5d\n",
    "        self.convd5_1 = layers.Conv2D(512,3,padding='same')\n",
    "        self.bnd5_1 = layers.BatchNormalization()\n",
    "        self.relud5_1 = layers.Activation('relu')\n",
    "        \n",
    "        self.convd5_2 = layers.Conv2D(512,3,padding='same')\n",
    "        self.bnd5_2 = layers.BatchNormalization()\n",
    "        self.relud5_2 = layers.Activation('relu')\n",
    "        \n",
    "        self.convd5_3 = layers.Conv2D(512,3,padding='same')\n",
    "        self.bnd5_3 = layers.BatchNormalization()\n",
    "        self.relud5_3 = layers.Activation('relu')\n",
    "        \n",
    "        #stage 4d\n",
    "        self.convd4_1 = layers.Conv2D(512,3,padding='same')\n",
    "        self.bnd4_1 = layers.BatchNormalization()\n",
    "        self.relud4_1 = layers.Activation('relu')\n",
    "        \n",
    "        self.convd4_2 = layers.Conv2D(512,3,padding='same')\n",
    "        self.bnd4_2 = layers.BatchNormalization()\n",
    "        self.relud4_2 = layers.Activation('relu')\n",
    "        \n",
    "        self.convd4_3 = layers.Conv2D(256,3,padding='same')\n",
    "        self.bnd4_3 = layers.BatchNormalization()\n",
    "        self.relud4_3 = layers.Activation('relu')\n",
    "        \n",
    "        #stage 3d\n",
    "        self.convd3_1 = layers.Conv2D(256,3,padding='same')\n",
    "        self.bnd3_1 = layers.BatchNormalization()\n",
    "        self.relud3_1 = layers.Activation('relu')\n",
    "        \n",
    "        self.convd3_2 = layers.Conv2D(256,3,padding='same')\n",
    "        self.bnd3_2 = layers.BatchNormalization()\n",
    "        self.relud3_2 = layers.Activation('relu')\n",
    "        \n",
    "        self.convd3_3 = layers.Conv2D(128,3,padding='same')\n",
    "        self.bnd3_3 = layers.BatchNormalization()\n",
    "        self.relud3_3 = layers.Activation('relu')\n",
    "        \n",
    "        #stage 2d\n",
    "        self.convd2_1 = layers.Conv2D(128,3,padding='same')\n",
    "        self.bnd2_1 = layers.BatchNormalization()\n",
    "        self.relud2_1 = layers.Activation('relu')\n",
    "        \n",
    "        self.convd2_2 = layers.Conv2D(128,3,padding='same')\n",
    "        self.bnd2_2 = layers.BatchNormalization()\n",
    "        self.relud2_2 = layers.Activation('relu')\n",
    "        \n",
    "        self.convd2_3 = layers.Conv2D(64,3,padding='same')\n",
    "        self.bnd2_3 = layers.BatchNormalization()\n",
    "        self.relud2_3 = layers.Activation('relu')\n",
    "        \n",
    "        #stage 1d\n",
    "        self.convd1_1 = layers.Conv2D(64,3,padding='same')\n",
    "        self.bnd1_1 = layers.BatchNormalization()\n",
    "        self.relud1_1 = layers.Activation('relu')\n",
    "        \n",
    "        self.convd1_2 = layers.Conv2D(64,3,padding='same')\n",
    "        self.bnd1_2 = layers.BatchNormalization()\n",
    "        self.relud1_2 = layers.Activation('relu')\n",
    "        \n",
    "        self.convd1_3 = layers.Conv2D(64,3,padding='same')\n",
    "        self.bnd1_3 = layers.BatchNormalization()\n",
    "        self.relud1_3 = layers.Activation('relu')\n",
    "        \n",
    "        #----------------------Bilinear upsampling-----------------\n",
    "        self.upscore6 = layers.UpSampling2D(32,interpolation='bilinear')###\n",
    "        self.upscore5 = layers.UpSampling2D(16,interpolation='bilinear')\n",
    "        self.upscore4 = layers.UpSampling2D(8,interpolation='bilinear')\n",
    "        self.upscore3 = layers.UpSampling2D(4,interpolation='bilinear')\n",
    "        self.upscore2 = layers.UpSampling2D(2, interpolation='bilinear')\n",
    "        \n",
    "         ## -------------Side Output--------------\n",
    "        self.outconvb = layers.Conv2D(1,3,padding='same')\n",
    "        self.outconv6 = layers.Conv2D(1,3,padding='same')\n",
    "        self.outconv5 = layers.Conv2D(1,3,padding='same')\n",
    "        self.outconv4 = layers.Conv2D(1,3,padding='same')\n",
    "        self.outconv3 = layers.Conv2D(1,3,padding='same')\n",
    "        self.outconv2 = layers.Conv2D(1,3,padding='same')\n",
    "        self.outconv1 = layers.Conv2D(1,3,padding='same')\n",
    "        \n",
    "        ## -------------Refine Module-------------\n",
    "        self.refunet = RefUnet(64)\n",
    "    \n",
    "    def call(self,x):\n",
    "        hx = x \n",
    "        #-----------------Encoder------------------\n",
    "        hx = self.inconv(hx)\n",
    "        hx = self.inbn(hx)\n",
    "        hx = self.inrelu(hx)\n",
    "        #----------------e1---------------\n",
    "        h1 = self.encoder1(hx)\n",
    "        #----------------e2---------------\n",
    "        h2 = self.encoder2(h1)\n",
    "        #----------------e3---------------\n",
    "        h3 = self.encoder3(h2)\n",
    "        #----------------e4---------------\n",
    "        h4 = self.encoder4(h3)\n",
    "        \n",
    "        hx = self.pool4(h4)\n",
    "        \n",
    "        \n",
    "        hx = self.resb5_1(hx)\n",
    "        hx = self.resb5_2(hx)\n",
    "        h5 = self.resb5_3(hx)\n",
    "        \n",
    "        hx = self.pool5(h5)\n",
    "        \n",
    "        hx = self.resb6_1(hx)\n",
    "        hx = self.resb6_2(hx)\n",
    "        h6 = self.resb6_3(hx)\n",
    "        \n",
    "        #-----------------------Bridge-------------------\n",
    "        \n",
    "        hx = self.relubg_1(self.bnbg_1(self.convbg_1(h6)))\n",
    "        hx = self.relubg_2(self.bnbg_2(self.convbg_2(hx)))\n",
    "        hbg = self.relubg_3(self.bnbg_3(self.convbg_3(hx)))\n",
    "        \n",
    "        #-----------------------Decoder--------------------\n",
    "        \n",
    "        hx = self.relud6_1(self.bnd6_1(self.convd6_1(layers.concatenate([hbg,h6],-1))))\n",
    "        hx = self.relud6_2(self.bnd6_2(self.convd6_2(hx)))\n",
    "        hd6 = self.relud6_3(self.bnd6_3(self.convd6_3(hx)))\n",
    "        \n",
    "        hx = self.upscore2(hd6)\n",
    "        \n",
    "        hx = self.relud5_1(self.bnd5_1(self.convd5_1(layers.concatenate([hx,h5],-1))))\n",
    "        hx = self.relud5_2(self.bnd5_2(self.convd5_2(hx)))\n",
    "        hd5 = self.relud5_3(self.bnd5_3(self.convd5_3(hx)))\n",
    "        \n",
    "        hx = self.upscore2(hd5)\n",
    "        \n",
    "        hx = self.relud4_1(self.bnd4_1(self.convd4_1(layers.concatenate([hx,h4],-1))))\n",
    "        hx = self.relud4_2(self.bnd4_2(self.convd4_2(hx)))\n",
    "        hd4 = self.relud4_3(self.bnd4_3(self.convd4_3(hx)))\n",
    "        \n",
    "        hx = self.upscore2(hd4)\n",
    "        \n",
    "        hx = self.relud3_1(self.bnd3_1(self.convd3_1(layers.concatenate([hx,h3],-1))))\n",
    "        hx = self.relud3_2(self.bnd3_2(self.convd3_2(hx)))\n",
    "        hd3 = self.relud3_3(self.bnd3_3(self.convd3_3(hx)))\n",
    "        \n",
    "        hx = self.upscore2(hd3)\n",
    "        \n",
    "        hx = self.relud2_1(self.bnd2_1(self.convd2_1(layers.concatenate([hx,h2],-1))))\n",
    "        hx = self.relud2_2(self.bnd2_2(self.convd2_2(hx)))\n",
    "        hd2 = self.relud2_3(self.bnd2_3(self.convd2_3(hx)))\n",
    "        \n",
    "        hx = self.upscore2(hd2)\n",
    "        \n",
    "        hx = self.relud1_1(self.bnd1_1(self.convd1_1(layers.concatenate([hx,h1],-1))))\n",
    "        hx = self.relud1_2(self.bnd1_2(self.convd1_2(hx)))\n",
    "        hd1 = self.relud1_3(self.bnd1_3(self.convd1_3(hx)))\n",
    "        \n",
    "        #------------------------------side output------------------------\n",
    "        \n",
    "        db = self.outconvb(hbg)\n",
    "        db = self.upscore6(db)\n",
    "        \n",
    "        d6 = self.outconv6(hd6)\n",
    "        d6 = self.upscore6(d6)\n",
    "        \n",
    "        d5 = self.outconv5(hd5)\n",
    "        d5 = self.upscore5(d5)\n",
    "        \n",
    "        d4 = self.outconv4(hd4)\n",
    "        d4 = self.upscore4(d4)\n",
    "        \n",
    "        d3 = self.outconv3(hd3)\n",
    "        d3 = self.upscore3(d3)\n",
    "        \n",
    "        d2 = self.outconv2(hd2)\n",
    "        d2 = self.upscore2(d2)\n",
    "        \n",
    "        d1 = self.outconv1(hd1)\n",
    "        \n",
    "        #-----------------------------Refine Module--------------\n",
    "        dout = self.refunet(d1)\n",
    "        \n",
    "        return tf.sigmoid(dout), tf.sigmoid(d1), tf.sigmoid(d2), tf.sigmoid(d3), tf.sigmoid(d4), tf.sigmoid(d5), tf.sigmoid(d6), tf.sigmoid(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680f091",
   "metadata": {},
   "source": [
    "# Define losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dd42751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(y_true ,y_pred):\n",
    "    y_true_flat = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_flat = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_flat*y_pred_flat)\n",
    "    union = tf.keras.backend.sum(y_true_flat + y_pred_flat - y_true_flat*y_pred_flat)\n",
    "    \n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b32a27f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_loss(y_true ,y_pred):\n",
    "    return 1 - iou(y_true ,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b6eb340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossentropy(y_true ,y_pred):\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    return bce(y_true ,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed8d1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_loss(y_true,y_pred):\n",
    "    return 1 - tf.image.ssim(y_true,y_pred,max_val=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27b2d86",
   "metadata": {},
   "source": [
    "# Training  the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7145b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths \n",
    "import os \n",
    "train_dir = os.path.join('data','DUTS-TR')\n",
    "image_dir = os.path.join(train_dir,'DUTS-TR-Image')\n",
    "mask_dir = os.path.join(train_dir,'DUTS-TR-Mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ebd0cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path,isMask=False):\n",
    "    img = tf.keras.utils.load_img(path)\n",
    "    img = tf.image.resize(img,(224,224))\n",
    "    img = tf.keras.utils.img_to_array(img)\n",
    "    img = img / 255.0\n",
    "    if isMask:\n",
    "        img = tf.image.rgb_to_grayscale(img)\n",
    "        \n",
    "        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "907e89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [load_img(os.path.join(image_dir,i)) for i in os.listdir(image_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "490b07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = [load_img(os.path.join(mask_dir,i),True) for i in os.listdir(mask_dir)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b15d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "masks = np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2747b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "datagen.fit(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86bb4519",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33d6fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_cb = tf.keras.callbacks.EarlyStopping('accuracy',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30fd8b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "             loss=[iou_loss,binary_crossentropy,ssim_loss],\n",
    "             metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0be1f429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5277/5276 [==============================] - ETA: 0s - loss: 1.1069 - output_1_loss: 0.4395 - output_2_loss: 0.3892 - output_3_loss: 0.2782 - output_1_accuracy: 0.6073 - output_2_accuracy: 0.6451 - output_3_accuracy: 0.6437 - output_4_accuracy: 0.0118 - output_5_accuracy: 0.1248 - output_6_accuracy: 0.5863 - output_7_accuracy: 0.4546 - output_8_accuracy: 0.2460WARNING:tensorflow:Early stopping conditioned on metric `accuracy` which is not available. Available metrics are: loss,output_1_loss,output_2_loss,output_3_loss,output_1_accuracy,output_2_accuracy,output_3_accuracy,output_4_accuracy,output_5_accuracy,output_6_accuracy,output_7_accuracy,output_8_accuracy\n",
      "5276/5276 [==============================] - 2870s 542ms/step - loss: 1.1069 - output_1_loss: 0.4395 - output_2_loss: 0.3892 - output_3_loss: 0.2782 - output_1_accuracy: 0.6073 - output_2_accuracy: 0.6451 - output_3_accuracy: 0.6437 - output_4_accuracy: 0.0118 - output_5_accuracy: 0.1248 - output_6_accuracy: 0.5863 - output_7_accuracy: 0.4546 - output_8_accuracy: 0.2460\n",
      "Epoch 2/10\n",
      "5277/5276 [==============================] - ETA: 0s - loss: 0.7943 - output_1_loss: 0.3176 - output_2_loss: 0.2779 - output_3_loss: 0.1988 - output_1_accuracy: 0.6532 - output_2_accuracy: 0.6613 - output_3_accuracy: 0.6644 - output_4_accuracy: 0.0022 - output_5_accuracy: 0.1207 - output_6_accuracy: 0.6198 - output_7_accuracy: 0.4562 - output_8_accuracy: 0.2661WARNING:tensorflow:Early stopping conditioned on metric `accuracy` which is not available. Available metrics are: loss,output_1_loss,output_2_loss,output_3_loss,output_1_accuracy,output_2_accuracy,output_3_accuracy,output_4_accuracy,output_5_accuracy,output_6_accuracy,output_7_accuracy,output_8_accuracy\n",
      "5276/5276 [==============================] - 2863s 543ms/step - loss: 0.7943 - output_1_loss: 0.3176 - output_2_loss: 0.2779 - output_3_loss: 0.1988 - output_1_accuracy: 0.6532 - output_2_accuracy: 0.6613 - output_3_accuracy: 0.6644 - output_4_accuracy: 0.0022 - output_5_accuracy: 0.1207 - output_6_accuracy: 0.6198 - output_7_accuracy: 0.4562 - output_8_accuracy: 0.2661\n",
      "Epoch 3/10\n",
      "5277/5276 [==============================] - ETA: 0s - loss: 0.6826 - output_1_loss: 0.2759 - output_2_loss: 0.2378 - output_3_loss: 0.1689 - output_1_accuracy: 0.6626 - output_2_accuracy: 0.6675 - output_3_accuracy: 0.6694 - output_4_accuracy: 0.0010 - output_5_accuracy: 0.1795 - output_6_accuracy: 0.6264 - output_7_accuracy: 0.4405 - output_8_accuracy: 0.2651WARNING:tensorflow:Early stopping conditioned on metric `accuracy` which is not available. Available metrics are: loss,output_1_loss,output_2_loss,output_3_loss,output_1_accuracy,output_2_accuracy,output_3_accuracy,output_4_accuracy,output_5_accuracy,output_6_accuracy,output_7_accuracy,output_8_accuracy\n",
      "5276/5276 [==============================] - 2861s 542ms/step - loss: 0.6826 - output_1_loss: 0.2759 - output_2_loss: 0.2378 - output_3_loss: 0.1689 - output_1_accuracy: 0.6626 - output_2_accuracy: 0.6675 - output_3_accuracy: 0.6694 - output_4_accuracy: 0.0010 - output_5_accuracy: 0.1795 - output_6_accuracy: 0.6264 - output_7_accuracy: 0.4405 - output_8_accuracy: 0.2651\n",
      "Epoch 4/10\n",
      "5277/5276 [==============================] - ETA: 0s - loss: 0.6313 - output_1_loss: 0.2564 - output_2_loss: 0.2200 - output_3_loss: 0.1549 - output_1_accuracy: 0.6674 - output_2_accuracy: 0.6708 - output_3_accuracy: 0.6728 - output_4_accuracy: 0.0013 - output_5_accuracy: 0.1963 - output_6_accuracy: 0.6305 - output_7_accuracy: 0.4508 - output_8_accuracy: 0.2508WARNING:tensorflow:Early stopping conditioned on metric `accuracy` which is not available. Available metrics are: loss,output_1_loss,output_2_loss,output_3_loss,output_1_accuracy,output_2_accuracy,output_3_accuracy,output_4_accuracy,output_5_accuracy,output_6_accuracy,output_7_accuracy,output_8_accuracy\n",
      "5276/5276 [==============================] - 2865s 543ms/step - loss: 0.6313 - output_1_loss: 0.2564 - output_2_loss: 0.2200 - output_3_loss: 0.1549 - output_1_accuracy: 0.6674 - output_2_accuracy: 0.6708 - output_3_accuracy: 0.6728 - output_4_accuracy: 0.0013 - output_5_accuracy: 0.1963 - output_6_accuracy: 0.6305 - output_7_accuracy: 0.4508 - output_8_accuracy: 0.2508\n",
      "Epoch 5/10\n",
      "5277/5276 [==============================] - ETA: 0s - loss: 0.5856 - output_1_loss: 0.2381 - output_2_loss: 0.2034 - output_3_loss: 0.1442 - output_1_accuracy: 0.6713 - output_2_accuracy: 0.6741 - output_3_accuracy: 0.6752 - output_4_accuracy: 0.0022 - output_5_accuracy: 0.2068 - output_6_accuracy: 0.6367 - output_7_accuracy: 0.4483 - output_8_accuracy: 0.2770WARNING:tensorflow:Early stopping conditioned on metric `accuracy` which is not available. Available metrics are: loss,output_1_loss,output_2_loss,output_3_loss,output_1_accuracy,output_2_accuracy,output_3_accuracy,output_4_accuracy,output_5_accuracy,output_6_accuracy,output_7_accuracy,output_8_accuracy\n",
      "5276/5276 [==============================] - 2864s 543ms/step - loss: 0.5856 - output_1_loss: 0.2381 - output_2_loss: 0.2034 - output_3_loss: 0.1442 - output_1_accuracy: 0.6713 - output_2_accuracy: 0.6741 - output_3_accuracy: 0.6752 - output_4_accuracy: 0.0022 - output_5_accuracy: 0.2068 - output_6_accuracy: 0.6367 - output_7_accuracy: 0.4483 - output_8_accuracy: 0.2770\n",
      "Epoch 6/10\n",
      "5277/5276 [==============================] - ETA: 0s - loss: 0.5531 - output_1_loss: 0.2247 - output_2_loss: 0.1912 - output_3_loss: 0.1372 - output_1_accuracy: 0.6731 - output_2_accuracy: 0.6751 - output_3_accuracy: 0.6766 - output_4_accuracy: 0.0032 - output_5_accuracy: 0.2178 - output_6_accuracy: 0.6381 - output_7_accuracy: 0.4317 - output_8_accuracy: 0.2771WARNING:tensorflow:Early stopping conditioned on metric `accuracy` which is not available. Available metrics are: loss,output_1_loss,output_2_loss,output_3_loss,output_1_accuracy,output_2_accuracy,output_3_accuracy,output_4_accuracy,output_5_accuracy,output_6_accuracy,output_7_accuracy,output_8_accuracy\n",
      "5276/5276 [==============================] - 2865s 543ms/step - loss: 0.5531 - output_1_loss: 0.2247 - output_2_loss: 0.1912 - output_3_loss: 0.1372 - output_1_accuracy: 0.6731 - output_2_accuracy: 0.6751 - output_3_accuracy: 0.6766 - output_4_accuracy: 0.0032 - output_5_accuracy: 0.2178 - output_6_accuracy: 0.6381 - output_7_accuracy: 0.4317 - output_8_accuracy: 0.2771\n",
      "Epoch 7/10\n",
      "3597/5276 [===================>..........] - ETA: 15:13 - loss: 0.5309 - output_1_loss: 0.2158 - output_2_loss: 0.1829 - output_3_loss: 0.1323 - output_1_accuracy: 0.6737 - output_2_accuracy: 0.6755 - output_3_accuracy: 0.6766 - output_4_accuracy: 0.0031 - output_5_accuracy: 0.1932 - output_6_accuracy: 0.6404 - output_7_accuracy: 0.4217 - output_8_accuracy: 0.2727"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\MACHINELEARNING\\TENSORFLOW_P\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mE:\\MACHINELEARNING\\TENSORFLOW_P\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:1414\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1412\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1414\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1416\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mE:\\MACHINELEARNING\\TENSORFLOW_P\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 438\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\MACHINELEARNING\\TENSORFLOW_P\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    295\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 297\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mE:\\MACHINELEARNING\\TENSORFLOW_P\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    316\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    321\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32mE:\\MACHINELEARNING\\TENSORFLOW_P\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    355\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    359\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mE:\\MACHINELEARNING\\TENSORFLOW_P\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1034\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\MACHINELEARNING\\TENSORFLOW_P\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1105\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1106\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1107\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mE:\\MACHINELEARNING\\TENSORFLOW_P\\tensorflow\\lib\\site-packages\\keras\\utils\\tf_utils.py:607\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    605\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\MACHINELEARNING\\TENSORFLOW_P\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mE:\\MACHINELEARNING\\TENSORFLOW_P\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mE:\\MACHINELEARNING\\TENSORFLOW_P\\tensorflow\\lib\\site-packages\\keras\\utils\\tf_utils.py:601\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    599\u001b[0m   \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 601\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m   \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mE:\\MACHINELEARNING\\TENSORFLOW_P\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1159\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \n\u001b[0;32m   1138\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1159\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mE:\\MACHINELEARNING\\TENSORFLOW_P\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1125\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1126\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(datagen.flow(images,masks,batch_size=2),steps_per_epoch=len(images)/2,epochs=10,callbacks=[es_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "685a4175",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrained_model_11-08.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\MACHINELEARNING\\TENSORFLOW_P\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mE:\\MACHINELEARNING\\TENSORFLOW_P\\tensorflow\\lib\\site-packages\\keras\\saving\\save.py:142\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (save_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh5\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     (h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile)) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     saving_utils\u001b[38;5;241m.\u001b[39mis_hdf5_filepath(filepath)):\n\u001b[0;32m    139\u001b[0m   \u001b[38;5;66;03m# TODO(b/130258301): add utility method for detecting model type.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_is_graph_network \u001b[38;5;129;01mand\u001b[39;00m  \u001b[38;5;66;03m# pylint:disable=protected-access\u001b[39;00m\n\u001b[0;32m    141\u001b[0m       \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, sequential\u001b[38;5;241m.\u001b[39mSequential)):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaving the model to HDF5 format requires the model to be a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFunctional model or a Sequential model. It does not work for \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubclassed models, because such models are defined via the body of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma Python method, which isn\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt safely serializable. Consider saving \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto the Tensorflow SavedModel format (by setting save_format=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mor using `save_weights`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    149\u001b[0m   hdf5_format\u001b[38;5;241m.\u001b[39msave_model_to_hdf5(\n\u001b[0;32m    150\u001b[0m       model, filepath, overwrite, include_optimizer)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."
     ]
    }
   ],
   "source": [
    "model.save('trained_model_11-08.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "985b9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_weight_.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01bc3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = os.path.join('data','DUTS-TE','DUTS-TE-Image','ILSVRC2012_test_00000003.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e51938bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(img_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4448279e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bb895d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(np.expand_dims(img,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3a73081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 1, 224, 224, 1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pred).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507fde52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10(tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
